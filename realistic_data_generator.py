from faker import Faker
from datetime import datetime, timedelta
import random
import pandas as pd
from spark_configurations import create_configured_session
import yfinance as yf
from concurrent.futures import ThreadPoolExecutor, as_completed

def get_initial_prices() -> dict[str, float]:
    """Real initial prices from early 2022"""
    return {
        "AAPL": 182.01,  # Jan 3, 2022
        "GOOGL": 144.42,  # (Post-split price)
        "AMZN": 170.15,  # (Post-split price)
        "MSFT": 334.75,
        "TSLA": 399.93   # (Post-split price)
    }

def generate_realistic_stock_data(year) -> pd.DataFrame:
    # initialize the data generation
    start_date = datetime(year, 1, 1)
    end_date = datetime(year, 12, 31)
    """Generate realistic stock data with hourly intervals"""
    trading_hours = range(9, 17)  # 9 AM to 4 PM
    initial_prices: dict[str, float] = get_initial_prices()

    data: list = []

    for stock, initial_price in initial_prices.items():
        # Get real volatility from Yahoo Finance
        ticker = yf.Ticker(stock)
        hist: pd.DataFrame = ticker.history(period="1y")
        real_volatility: float = hist['Close'].pct_change().std()

        current_price: float = initial_price
        current_date = start_date

        while current_date <= end_date:
            # Skip weekends
            if current_date.weekday() >= 5:
                current_date += timedelta(days=1)
                continue

            for hour in trading_hours:
                timestamp = current_date.replace(hour=hour, minute=0)

                # Daily volatility is typically lower than total volatility
                daily_volatility = real_volatility * 0.6

                # Generate realistic price movement
                price_change = random.gauss(0, daily_volatility) * current_price
                current_price += price_change

                # Calculate high, low, and volume
                high = current_price * (1 + random.uniform(0, 0.002))
                low = current_price * (1 - random.uniform(0, 0.002))
                volume = int(random.gauss(1000000, 300000))  # Realistic volume

                data.append({
                    'timestamp': timestamp,
                    'symbol': stock,
                    'open': round(current_price, 2),
                    'high': round(high, 2),
                    'low': round(low, 2),
                    'close': round(current_price + random.uniform(-0.1, 0.1), 2),
                    'volume': max(0, volume),
                    'vwap': round((high + low + current_price) / 3, 2)  # Volume Weighted Average Price
                })

            current_date += timedelta(days=1)

    return pd.DataFrame(data)


"""
Explanation: Different file generation using the pyspark
    stock_data/ directory: This is the output directory created by PySpark when saving parquet files.
    Parquet Directory Structure
    Directory Structure Explanation:
1. _SUCCESS file:
    Empty file
    Created when Spark successfully completes writing all data
    Used as a flag to confirm data completeness
2. symbol=XXX directories:
    Created due to .partitionBy("symbol") in the code
    Each directory contains data for one stock symbol
    Improves query performance when filtering by symbol
3. part-XXXXX files:
Actual parquet data files
Number of files depends on:
    Number of partitions
    Data size
    spark.sql.shuffle.partitions setting
"""


"""
Why the .crc file get generated when parquet file is written?
Let me explain the .crc files in Spark/Hadoop's file system:

### Understanding .crc Files

The .crc (Cyclic Redundancy Check) files are automatically generated by Hadoop/Spark to ensure data integrity. Here's a typical directory structure with .crc files:

```
stock_data/
├── .part-00000-xxx.parquet.crc
├── part-00000-xxx.parquet
├── .part-00001-xxx.parquet.crc
├── part-00001-xxx.parquet
├── ._SUCCESS.crc
└── _SUCCESS
```

Key points about .crc files:

1. **Purpose**:
- Data integrity verification
- Corruption detection
- Ensures file completeness

2. **How they work**:
```python
# Example of how Spark uses CRC
def verify_file_integrity(file_path):
    actual_file = f"part-00000-xxx.parquet"
    crc_file = f".part-00000-xxx.parquet.crc"

    # Spark automatically performs this check when reading files
    if computed_checksum != stored_checksum_in_crc:
        raise Exception("Data corruption detected")
```

3. **Properties**:
- Small binary files (typically 4-8 bytes)
- One .crc file per data file
- Hidden files (start with '.')
- Generated automatically during write operations

4. **Benefits**:
- Ensures data reliability
- Detects file corruption
- Validates successful writes
- Part of Hadoop's fault tolerance

You can safely ignore these files as they are managed automatically by Spark/Hadoop. They're essential for data integrity but transparent to your application code.

"""

def save_to_parquet(df, spark, output_path="stock_data"):
    """Save the data using Spark"""
    spark_df = spark.createDataFrame(df)
    spark_df.write.mode("overwrite").partitionBy("symbol").parquet(output_path)
    print(f"Data saved to {output_path}")
    return spark_df

def parallel_data_generator() -> pd.DataFrame:
    """
    Use the concurrent.futures to run the data generation in parallel, Number of threads = 4
    """
    years: list = [2023, 2024]
    all_data: list[pd.DataFrame] = []

    with ThreadPoolExecutor(max_workers=4) as executor:
        # Submit Task for each year
        future_to_year = {
            executor.submit(generate_realistic_stock_data, year): year for year in years
        }


    # collect results as they completed

    for future in as_completed(future_to_year):
        year = future_to_year[future]
        try:
            data: pd.DataFrame = future.result()
            all_data.append(data)
            print(f"Data generated for {year}")
        except Exception as e:
            print(f"Error generating data for {year}: {e}")

    return pd.concat(all_data, ignore_index=True)



def main() -> None:


    """Generate and save realistic stock data"""
    # Generate data using parallel processing
    df: pd.DataFrame = parallel_data_generator()

    # Basic statistics
    print("\nDataset Statistics:")
    print(f"Total records: {len(df)}")
    print("\nSample data:")
    print(df.head())

    # Save data using Spark
    spark = create_configured_session()
    spark_df = save_to_parquet(df, spark)

    # Show data distribution
    print("\nData distribution by symbol:")
    spark_df.groupBy("symbol").count().show()

    spark.stop()

if __name__ == "__main__":
    main()
